Avatar customization is known to positively affect crucial outcomes in numerous domains. However, it is unknown whether audial customization can confer the same benefits as visual customization. We conducted a preregistered 2 x 2 (visual choice vs. visual assignment x audial choice vs. audial assignment) study in a Java programming game. Participants with visual choice experienced higher avatar identification and autonomy. Participants with audial choice experienced higher avatar identification and autonomy, but only within the group of participants who had visual choice available. Visual choice led to an increase in time spent, and indirectly led to increases in intrinsic motivation, immersion, time spent, future play motivation, and likelihood of game recommendation. Audial choice moderated the majority of these effects. Our results suggest that audial customization plays an important enhancing role vis-à-vis visual customization. However, audial customization appears to have a weaker effect compared to visual customization. We discuss the implications for avatar customization more generally across digital applications. 
 ACM Reference Format: Dominic Kao, Rabindra Ratan, Christos Mousas, Amogh Joshi, and Edward F. Melcer. 2022. Audio Matters Too: How Audial Avatar Customization Enhances Visual Avatar Customization. In CHI Conference on Human Factors in Computing Systems (CHI '22), April 29-May 5, 2022, New Orleans, LA, USA. ACM, New York, NY, USA 27 Pages. https://doi.org/10.1145/3491102.3501848
Avatars are ubiquitous across digital applications. Using avatars as representations of ourselves, we socialize, play, and work. Increasingly, researchers have become interested in avatar customization . Given prior work demonstrating the importance of avatar customization and audio separately, allowing players to audially customize their avatars may have beneficial effects.
Customizing one's avatar is often viewed as inherently enjoyable . For the purposes of the present study, we created four character models and four character voices. We then created four character customization interfaces that varied (1) whether the character model was chosen or randomly assigned and (2) whether the character voice was chosen or randomly assigned. These customization interfaces were explicitly designed to test whether audial customization would have any effect on outcomes vis-à-vis visual customization.
We conducted an online study on Amazon's Mechanical Turk (MTurk) in which participants were randomly assigned to one of the four character customization interfaces. Participants then played a Java programming game for 10 minutes. After 10 minutes had passed, an in-game survey collected measures of avatar identification, autonomy, intrinsic motivation, immersion, motivated behavior, motivation for future play, and likelihood of game recommendation.1 After completing the survey, participants could quit or continue playing for as long as they liked, reflecting motivated behavior.
Our results show that visual customization leads to higher avatar identification and autonomy. Audial customization leads to higher avatar identification and autonomy, but only within the grouping of participants in which visual customization was available. In the grouping of participants without visual customization, audial customization had no effect on avatar identification or autonomy. Visual customization leads to higher time spent playing, and indirectly (through the mediators of avatar identification and autonomy), it leads to higher intrinsic motivation, immersion, time spent playing, motivation for future play, and likelihood of game recommendation. Audial customization moderated the direct effect of visual customization on time spent playing, as well as the indirect effects of visual customization on intrinsic motivation, immersion, motivation for future play, and likelihood of game recommendation. The moderation effect was such that the effect was non-significant when audial customization was unavailable but significant when audial customization was available. Our results show that audial customization, although having an overall weaker effect than visual customization, can strengthen existing effects of visual customization on outcomes. This suggests that avatar customization systems in games can be improved by adding audial customization options. Moreover, our study provides motivation to extend this research to other domains as potential beneficiaries of audial avatar customization (e.g., virtual reality, digital learning, health applications). In the highly understudied area of avatar audio, we contribute baseline results in a large-scale preregistered study that can spur further work in this domain.
Avatar customization is the process of changing aspects of a video game character. Players customize their avatars’ physical (e.g., body shape), demographic (e.g., age, race, gender), and transient (e.g., clothes, ornaments) aspects. The avatar customization process can also include choosing roles (e.g., playing as a warrior, archer, mage, or a healer), attributes (e.g., luck, intelligence), and group membership (e.g., playing as horde or alliance) . The study found that players who customized their avatars had a stronger identification and expressed greater empathy towards them than those who played the game with premade avatars.
Studies have also used bespoke games to understand the effects of avatar customization .
These studies suggest that avatar customization affects player experience in a wide variety of settings (e.g., games for entertainment or learning), virtual environments (e.g., desktop, VR) and timespans (both one-off play sessions and longitudinal) .
2.1.1 Avatar Identification. Identification is a mechanism wherein media experiences—such as reading a story or watching a movie—are interpreted and experienced by audiences as if “the events were happening to them” .
Avatar identification is thought to be a shift in self-perception .
One avenue of understanding identification is through understanding the avatar customization process. When players customize their avatar, they cycle through many “possible selves” .
These findings have led researchers to consider avatar identification as a multi-faceted construct .
The process of avatar customization is often a precursor for generating greater avatar identification. For example, players wanting to create an avatar that has similar attributes (e.g., physical appearance, hair style, hair color) may generate greater similarity identification .
2.1.2 Avatar Customization Interface. The interface that the players use to create and customize their avatars—sometimes referred to as a character customization interface (CCI) .
For instance, the design of “default” options in avatar customization interfaces and the order (hierarchy) of body customization options oftentimes implicitly reinforces existing hegemonic structures in society . While our focus in the present study is on understanding if audial avatar customization can confer similar benefits to visual avatar customization, the exclusionary potential of audial avatar customization options should be studied closely in future research.
Research has emphasized the role played by other aspects including game world aesthetics, co-situated players, social context, and avatars of other characters in influencing the avatar customization process .
Although the process of avatar customization has been extensively investigated, research has largely ignored the effect of voice options on avatar creation and customization. Contemporary games seldom offer voice customization options; however, there do exist some examples. Some games offer a “voice template” that can be chosen during avatar customization, such as in Black Desert Online  offer the ability to modify pitch. This project investigates the effect of providing audial avatar customization options on a variety of player outcomes.
Game audio performs many functions, such as emphasizing visuals  classifies sounds into three categories: speech and dialogue, sound effects (e.g., ambient noise, avatar sounds, object and ornamental sounds), and music.
Research shows that players appreciate the inclusion of audio elements in the game. Klimmt et al. . Players experienced a lower degree of perceived game atmosphere when the audial elements did not fit the game's visual elements.
Avatar sounds are sounds related to the avatar activity, such as breathing and footstep sounds .
2.2.1 Avatar Voice. Avatar voice includes linguistic (e.g., dialogue and voiceovers) and non-linguistic vocalizations such as emotes (e.g., effort grunts, screams, sighs) .
Voice dialogue in games supports storytelling, the development of a rich and believable world, and setting emotional tone . For instance, an urgent request for help can arouse the player to take action.
Voice interaction focuses on using players’ voices as input in the game .
Carter suggests that voice interaction can facilitate a deeper connection with the players’ game characters .
2.2.2 Avatar Voice and Learning Environments. Studies have investigated how engagement and learning outcomes are influenced by voice characteristics of the instructional agents .
More recently, research has also sought to understand how an avatar's voice can affect self-presentation in digital environments. Zhang et al. characterized users’ voice customization preferences on social media websites .
While research provides strong support for avatar voice influencing avatar identification, no study (to the best of our knowledge) has investigated the effects of providing avatar audial customization options. We present a study that provides audial (voice) avatar customization options alongside visual avatar customization options in a Java programming game. Our goal is to understand how providing audial avatar customization options affect measured outcomes.
We had seven overarching hypotheses (each broken down into three sub-hypotheses) in this study. All hypotheses and research questions were part of the study preregistration.2 Because prior work has shown that avatar customization leads to an increase in avatar identification (similarity identification, embodied identification, and wishful identification) . Therefore, we hypothesized that audial customization would lead to an increase in avatar identification. Additionally, we hypothesized a lack of an interaction effect between visual and audial customization because existing work gives us no reason to believe their effects would depend on one another.
Prior studies have shown that character customization leads to greater autonomy . Therefore, we hypothesized that visual customization would lead to greater autonomy. Similar to H1.2, we hypothesized that audial customization will play a similar role to visual customization and will also increase autonomy. We again hypothesized a lack of an interaction effect for the same reason as H1.3.
Prior work has shown that avatar customization is linked to intrinsic motivation . Therefore, we hypothesized a model in which visual customization directly, and indirectly through avatar identification and autonomy, influences intrinsic motivation, immersion, time spent playing, motivation for future play, and likelihood of game recommendation. Lastly, given the lack of prior work on audial customization, we posed as research questions (without any formal hypotheses) whether audial customization moderated any of these effects.
Research Question: Does audial customization moderate H3–H7? 
Shows two screenshots from CodeBreakers. First screenshot shows three bugs, each bug associated with a different data type (int, String, boolean) but with a missing value which the player must fill in. Second screenshot shows the code of a knight who is suffering from poison, with a loop which is hurting the knight on every iteration. Player must find the code to stop the poison from killing the knight.
Our experimental testbed is CodeBreakers4 . Programming topics include data types, conditionals and control flow, classes and objects, inheritance and interfaces, loops and recursion, and data structures. Each puzzle had up to 3 hints, which are increasingly detailed. Players controlled their character using the keyboard and mouse. CodeBreakers was originally developed for Microsoft Windows and macOS. However, for the purposes of this experiment, CodeBreakers was converted to WebGL and was therefore playable on any PC inside of the browser (e.g., Chrome, Firefox, Safari). See Section 4.4.1 for details. In total, there were 30 possible voice lines that could have been triggered. Other than the first voice line (What am I doing here? Did my ship crash? How long have I been lying here for? I guess I should get up and look around.), audio lines typically come before and after each puzzle. For example, prior to puzzle #7: The castle is under siege!. And after completing puzzle #7: It worked! I neutralized all of the bugs by using the staff. These voice lines were accompanied by speech bubbles (see Figure 2). 
Shows two screenshots from CodeBreakers. Both screenshots have a speech bubble coming from the player and show the situations in which voice audio is played. The first speech bubble says ”Yes! I managed the fix the last staircase!” while the second says ”Yes! I've successfully defended the castle! My work is done here.”
For this study, we explicitly aimed to create stereotypically-appearing (and sounding) “male” and “female” avatars. We created four avatar appearances (two male and two female) and four avatar voices (two male and two female). We made these design decisions with an understanding that a binary view of gender is problematic, but we did so for ecological validity with the majority of existing games. While it would have been possible to create a more inclusive set of gender choices, this might present as a possible confound as such choices are not currently available in most of today's games. Our goal is to develop a baseline understanding of the presence of customization choices that mirror current games. Such baseline understandings can inform future avatar customization research and implementation, in which we hope that more inclusive design choices become the norm. Finally, our rationale for creating two visual choices and two audial choices for each gender was to add a (minimal) degree of visual and audial choice.
Shows the front and back view of the four gray avatars used in the game. All avatars look abstract, without specific facial feature details.
All four models used in this experiment were designed and created from scratch by a professional 3D game artist. The models were purposefully designed to avoid known color effects (e.g., the color red is known to reduce mood, affect, and performance in cognitive-oriented tasks . All four models shared the same identical skeleton and joints, and therefore all animations (i.e., idle, walking, picking up code, throwing code, using weapons, falling, dying, stopped in front of a wall, etc.) were identical across the four models. Only visual appearance differed. See Figure 3.
4.2.1 Voice Development Goal. Our goal was to create four avatar voices (two stereotypical male and two stereotypical female). We wanted each voice to be appropriate for the game and to be appropriate for either of the two models from the same gender. Additionally, we wanted each male voice to have a “matching” female voice as rated on a scale of perceived vocal dimensions—e.g., strong vs. weak, smooth vs. rough, resonant vs. shrill .5 In other words, we wanted these matched voices to sound as similar as possible. The reason this matching was done was to mitigate confounds from large differences between voices. High variance between voices would add an additional dimension to the manipulation which could influence the study results. Nevertheless, we wanted both male voices to be distinct from one another and both female voices to be distinct from one another. If this were not the case (e.g., both male voices sounded the same), then our manipulation of giving users a choice of voice would only be illusory.
4.2.2 Creating Voices. We hired two professional voice actors with over ten years of experience in character voice acting. Both voice actors were screened through their portfolios, which contained samples of their work. Both voice actors provided sample voice clips for CodeBreakers prior to being hired. We decided on hiring two voice actors instead of four because: (1) we could ensure greater overall consistency across voices, helping to bound the variance across voices and (2) both voice actors had demonstrated evidence of being able to perform a multitude of different voices and characters, assurance that each voice actor could produce two unique-sounding voices. Both voice actors self-identified as white and have lived in the U.S. for their entire lives. One voice actor self-identified as male and was 49 years old. The other voice actor self-identified as female and was 38 years old. The two voice actors were instructed to work together to create two “matching” voice pairs as described in Section 4.2.1. Our goals for the four voices, including the scale of vocal dimensions , were clearly articulated to the voice actors. Additionally, both voice actors familiarized themselves with the game by watching video gameplay of CodeBreakers. Both voice actors were also shown the four models that they were voicing. All voices were recorded in the same professional audio recording studio with both voice actors physically copresent. Identical recording equipment and software was used for recording each voice clip: Sennheiser MK-416 (microphone), Universal Audio Arrow (audio interface), and Ableton Live 10 (digital audio workstation). Completed voice clips were reviewed by the project team, and several iterations were made on the voice clips to ensure that our criteria in Section 4.2.1 appeared to be satisfied. A total of 120 voice clips (30 per voice) were recorded and finalized. Sample audio clips can be found at https://osf.io/mnpsd/. M1 is male voice one, M2 is male voice two, F1 is female voice one, and F2 is female voice two.
4.2.3 Voice Loudness Normalization. While the same identical recording studio and recording equipment was used for recording each voice, it is possible that relative amplitude (i.e., loudness) could differ between voices, especially between the two different voice actors. To normalize loudness across all voices and voice clips, we adopted the EBU R 128 (issued by the European Broadcasting Union) standard's recommendation for loudness normalization . It recommends normalization of audio to -23 ± 0.5 Loudness Units Full Scale (LUFS), and a max peak of -1 decibel True Peak (dBTP). A professional audio engineer with 15+ years of experience performed this normalization using Nuendo 11 Pro and verified that the loudness normalization recommendation was satisfied.
4.3.1 Expert Voice Validation.

To ensure that we had created two distinct matching pairs of voices (similarity within each pair but variance between them), we hired three expert speech pathologists to evaluate each voice. Each speech pathologist was given instructions to listen to a set of voices then asked to rate each voice on a scale. Each speech pathologist was compensated $25. Speech pathologists all had at least 10 years of professional speech pathology experience (M=20.0, SD=8.19), with an average age of M=47.67 (SD=4.93). Before rating the voices, each speech pathologist was instructed to familiarize themselves with the validated scale on perceptual attributes of voice ). Nevertheless, one potential concern arising from these results is that the same-gender voices may not be perceived as distinct from one another. Therefore, we performed an additional crowdsourced validation.
4.3.2 Crowdsourced Voice Validation. To ensure that we had created two distinct matching pairs of voices, that all voices would be perceived as as being high quality, that voices would be perceived as the stereotypical intended gender, and that voices across the same gender would be perceived as unique and distinct voices, we ran a crowdsourced validation study. This was to reinforce and extend the prior expert validation. We recruited 91 participants (39% self-identified as female) on MTurk to rate voices based on sets of audio clips. Each participant was compensated $1.00 (USD). Participants had a mean age of 40.62 (SD=13.82). All participants were from the U.S. After filling out a consent form, each participant was first presented with, randomly, either a stereotypical male or female voice clip of an English word, which they needed to type correctly. This was to ensure that the participant's audio was turned on and working. Each of the following questions was equipped with analytics that tracked the amount of time that each participant spent listening to audio clips. These analytics were used to validate that participants had actually listened to the audio clips before answering the questions. ~10% of participants were removed for not having listened to all audio clips in the study in their entirety.
Participants were then asked to “Please listen to ALL of the following audio clips before answering the question below comparing the first (left-side) and second (right-side) voices.” And to rate: “Besides gender-related voice characteristics, I consider these two voices as similar,” on a scale of 1:Strongly Disagree to 7:Strongly Agree. This question was asked four times comparing the following pairs of voices in a randomized order: M1/F1, M2/F2, M2/F1, and M1/F2. For each comparison, 5 voice clips were selected at random (from the total 30), and those same 5 voice clips were shown for both of the two voices being compared (i.e., the same speech dialog).7 Results indicated that matched pairs (M1/F1: M=5.51, SD=1.50; M2/F2: M=4.92, SD=1.68) were rated to be more highly similar to one another than unmatched pairs (M1/F2: M=4.01, SD=1.64; M2/F1: M=3.13, SD=1.71).
Participants were then asked to “Please listen to ALL of the following audio clips. All clips belong to one voice. After listening to all of the clips, you will be asked a question regarding the voice.” And to rate: “Based on the voice you just listened to, please rate the following: The voice is high-quality,” “The speaker sounds (stereotypically) male,” and “The speaker sounds (stereotypically) female” on a scale of 1:Strongly Disagree to 7:Strongly Agree. This question was asked for each of the four voices in randomized order. For each voice, 5 voice clips were selected at random (from the total 30). Results indicated that all voices were perceived to be relatively high quality (M1: M=6.02, SD=0.80; F1: M=6.06, SD=0.98; M2: M=5.80, SD=1.12; F2: M=5.60, SD=1.08) and that voices sounded stereotypically male (M1: M=6.74, SD=0.51; F1: M=1.20, SD=0.56; M2: M=6.85, SD=0.39; F2: M=1.34, SD=0.89) or female (M1: M=1.32, SD=0.77; F1: M=6.79, SD=0.44; M2: M=1.15, SD=0.52; F2: M=6.70, SD=0.55) as intended.
Participants were then asked to “Please listen to ALL of the following audio clips before answering the question below comparing the first (left-side) and second (right-side) voices.” And to rate: “In comparing the two voices above (left audio clips vs. right audio clips), please rate the following: These two voices are distinct and different from one another,” on a scale of 1:Strongly Disagree to 7:Strongly Agree. This question was asked twice for voices in each gender (M1/M2 and F1/F2) in a random order. For each comparison, 5 voice clips were selected at random. Results indicated that same-gender voice pairs were perceived to be relatively distinct (M1/M2: M=5.78, SD=1.07; F1/F2: M=5.73, SD=1.30). Participants then entered demographic information.
4.4.1 WebGL Conversion and Technical Testing.

Shows four screenshots, one of each condition. (a) Choice-None: Participant is randomly assigned both model and voice. (b) Choice-Audio: Participant is randomly assigned model and chooses voice. (c) Choice-Visual: Participant chooses a model and is randomly assigned voice. (d) Choice-All: Participant chooses both model and voice.
Over 4 months, the original CodeBreakers game, which is playable on machines running either Microsoft Windows or macOS , was converted to WebGL to allow for a more convenient play experience. The WebGL version is playable on any PC inside of the browser (e.g., Chrome, Firefox, Safari). This conversion was performed by a professional game development team with expertise in game optimization. During the conversion process, we iterated on the game internally every few days and externally every few weeks. Our main goal during these iterations was to ensure that performance (e.g., frames per second) was adequate and that there were no technical issues (e.g., crashing). Internal iterations were performed by the development and research team where feedback was fed into the next iteration. Performance profiling tools were used extensively to diagnose areas of the game (e.g., code loops, rendering of certain geometry) responsible for increased CPU and memory usage. External iterations were performed when we wanted the game to be tested more widely. We performed iterations with batches of 10-20 participants at a time on MTurk. Participants were asked to play the entire game and were provided a walkthrough video in case they were unable to progress. This ensured that each participant would cover the breadth of the entire game. Data, including gameplay metrics, performance, crash logs, and PC details, was automatically logged on the server for further analysis. Participants could report any issues, problems, or concerns they experienced during playtesting. A total of 121 participants, all from the U.S., took part in external playtesting. Each participant was compensated $10 (USD). Our testing ended when no new technical issues arose in the most recent internal and external iterations, all known technical issues were fixed, and the game performed adequately (e.g., frames per second, load times) under a wide variety of PCs. Additionally, the development and research team agreed that, for all intents and purposes, the WebGL game played and felt identical to the original. 
Model and voice validation summary graphs. Error bars show SD.
4.4.2 Character Customization UI. A professional game UI designer created four different character customization screens that we requested. These also correspond to our experimental conditions. (See Figure 4.) We made the explicit design decision never to allow mismatched model–voice gender pairings (i.e., male model and female voice or vice versa), since this may be unnatural for players, lacks general ecological validity with existing games, and may be an experimental confound (e.g., in conditions where one or both features are assigned at random). Therefore, avatar customization is, in all cases, a two-step process that involves first choosing or being assigned a model (one of four), then choosing or being assigned a voice (one of two since the model has already been selected, and there are only two voices corresponding to the designed stereotypical gender).
In Choice-None, the player does not have any choice over the model or voice. Both model and voice are randomly assigned. In Choice-Audio, a model is randomly preselected, and a player is able to choose the voice. In Choice-Visual, the player chooses a model, after which the voice is randomly assigned. In Choice-All, the player chooses both model and voice. Note that the two voices corresponding to “Voice 1” and “Voice 2” will differ depending on the model selected. In Choice-All, both voice options are grayed out and unavailable until a model has been selected. If a different model is selected after a voice has been selected, the voice is automatically deselected. In all conditions, players must enter a name for their character. For conditions that allow for a model choice (Choice-Visual and Choice-All), the UI initially shows an empty box where the selected model would normally appear (i.e., no model is selected by default). For conditions that allow for a voice choice (Choice-Audio and Choice-All), no voice is selected by default (i.e., one of the two voices must be selected manually by the player). When a voice is selected, a single audio clip is played from that voice so that players can compare voices. In all conditions, players must complete all customization options available (e.g., name, model, voice) before the “Start Game” button becomes available. Character customization conditions were designed in this manner to minimize differences between conditions, while still varying the manipulations (visual choice and audial choice).
4.4.3 Expert UI Validation. To assess the appropriateness of our character customization UIs, we performed a validation study with three professional game UI designers. Game UI designers were recruited from the online freelancing platform Upwork, and were each paid $20 (USD). The job posting was Assess Character Customization Interface in Educational Game, and the job description stated that we were looking for expert game UI designers to evaluate a set of character customization interfaces in an educational game. The three UI designers had an average of 9.00 (SD=4.36) years of UI design experience and an average of 7.67 (SD=5.69) years of game development experience. UI designers all had work experience and portfolios that reflected recent UI design and game development experience (all within one year). UI designers were instructed to give their honest opinions and were told their responses would be anonymous, and proceeded to our survey. Each UI designer was first asked to watch 30 minutes of gameplay footage from CodeBreakers to familiarize themselves with the game. Afterwards, each designer loaded CodeBreakers WebGL on their own machine and interacted with every version of the UI in a randomized order. After interacting with a specific version of the UI, the UI designer was asked to rate “The character customization interface is appropriate for the game,” on a scale of 1:Strongly Disagree to 7:Strongly Agree. UI designers were asked to rate each interface individually, not in comparison to the other interfaces they had already seen. UI designers were also able to report open-ended feedback. The survey took approximately 1.5 hours to complete. Responses showed that UI designers generally agreed that the character customization interface was appropriate (Choice-None: M=6.67, SD=0.58; Choice-Audial: M=6.33, SD=1.16; Choice-Visual: M=6.33, SD=1.16; Choice-All: M=7.00, SD=0.00). One UI designer did note as open-ended feedback that they had not expected to be able to choose a voice for their character since this is not a commonly available feature in games, but it was stated that this did not play a role in the designer's ratings.
4.4.4 Model and Voice Integration Validation. To assess whether the models and voices that we had developed would be perceived as appropriate for the game, we recruited 120 participants (43% female) on MTurk. All 120 participants played CodeBreakers using the Choice-None condition (i.e., randomly assigned model and voice). Participants played the game for a minimum of 5 minutes, but they were allowed to play as long as they liked beyond the 5-minute mark. Random assignments were roughly even across models (24.2%/24.2%/32.5%/19.2%) and voices (24.2%/27.5%/26.7%/21.7%). For the remainder of this section, ratings described for models follow the left-to-right order of models shown in Figure 3. See Figure 5 for graphs summarizing the validation results.8 
To assess whether models overall visually fit the game, we asked, “How appropriate were your avatar's visual characteristics for the game?” on a scale from 1:Inappropriate to 5:Appropriate. Scores tended between neutral and appropriate for each model (M=4.24, SD=0.83; M=3.86, SD=0.79; M=4.18, SD=0.76; M=4.04, SD=0.77). To assess whether voices overall audially fit the game, we asked “How appropriate was your avatar's voice for the game?” on a scale from 1:Inappropriate to 5:Appropriate. Scores again tended between neutral and appropriate for each voice (M1: M=3.91, SD=0.82; M2: M=4.46, SD=0.65; F1: M=4.55, SD=0.57; F2: M=3.97, SD=0.98). To assess whether models and voices in combination fit the game, we asked, “How appropriate were both the visual and audial characteristics combined of your avatar for the game?” on a scale from 1:Inappropriate to 5:Appropriate. Scores again tended between neutral and appropriate for each model (M=4.21, SD=0.77; M=4.00, SD=0.80; M=4.31, SD=0.69; M=4.04, SD=0.93) and for each voice (M1: M=3.88, SD=0.79; M2: M=4.39, SD=0.70; F1: M=4.35, SD=0.67; F2: M=4.09, SD=0.88). To assess whether models’ individual visual features (color and clothing) were appropriate for the avatar, and for the game, we asked, “How appropriate was the avatar color for the game?”, “How appropriate was the avatar color for the avatar?”, “How appropriate was the avatar clothing for the game?”, “How appropriate was the avatar clothing for the avatar?”, and “How appropriate was the avatar design overall?”, on a scale from 1:Inappropriate to 5:Appropriate. Overall scores were between neutral and appropriate for appropriateness of avatar color (Game: M=3.78, SD=1.00; Avatar: M=3.87, SD=1.02), avatar clothing (Game: M=4.08, SD=0.93; Avatar: M=4.17, SD=0.80), and avatar design overall (M=4.06, SD=0.87). To assess whether models were perceived as the stereotypical gender we had designed them to be, we asked, “I considered my avatar to be (stereotypically) male,” and “I considered my avatar to be (stereotypically) female,” on a scale from 1:Strongly Disagree to 5:Strongly Agree. Participants rated the models designed to be stereotypically male as male (M=4.66, SD=0.72; M=4.59, SD=0.95; M=1.62, SD=1.07; M=1.35, SD=0.94) and models designed to be stereotypically female as female (M=1.21, SD=0.49; M=1.38, SD=0.86; M=4.41, SD=0.97; M=4.74, SD=0.54). To assess whether avatars bore a visual similarity with players, we asked participants to rate “My avatar resembles me,” on a scale from 1:Strongly Disagree to 5:Strongly Agree. Participants who self-identified as male had scores tending towards neutral for male models (M=3.19, SD=0.98; M=2.64, SD=1.01; M=1.91, SD=1.06; M=1.55, SD=0.69) while participants who self-identified as female had scores tending towards neutral for female models (M=1.63, SD=0.92; M=2.07, SD=1.39; M=3.06, SD=1.20; M=3.67, SD=1.23). As expected, participants in general did not find close visual similarity with their avatars (likely in part due to their abstract design), with some natural variation across avatars and gender.
Our study was preregistered on the Open Science Framework (OSF). Hypotheses, exploratory analyses, experiment design, data collection, sample size, and measures are contained in our preregistration.9 
The study uses a 2 x 2 factorial design. We manipulate visual choice (choice vs. assignment) and audial choice (choice vs. assignment). The manipulations are as follows:
The only difference between each of these conditions is the character customization interface that appeared at the beginning of the game, which manipulated choice vs. assignment for model and voice. See Figure 4 and Section 4.4.2 for details on how the character customization interface was implemented in these different conditions. All other aspects of the experiment were identical across conditions.
In line with best practices on measurement reporting, we report what we are measuring, how we are measuring, and why are we measuring in this way .
4.7.1 Avatar Identification (Player Identification Scale). Avatar identification is a “temporary alteration of media users’ self-concept through adoption of perceived characteristics of a media person” .
4.7.2 Autonomy (Player Experience of Need Satisfaction). Autonomy is the sense that one has volition and is doing activities for interest and personal value .
4.7.3 Intrinsic Motivation (Intrinsic Motivation Inventory). Intrinsic motivation is one's willingness to engage in an activity because the activity is satisfying in and of itself .
4.7.4 Immersion (Player Experience Inventory). Immersion is a sense of immersion and cognitive absorption, experienced by the player .
4.7.5 Motivated Behavior (Time Played). We operationalize motivated behavior as the time spent playing the game. Time on task is a behavioral measure that has been linked to motivation  and is an objective measure of motivation in this study. Note that in the current study, participants are required to play at least 10 minutes, after which playing longer is optional.
4.7.6 Motivation For Future Play and Likelihood of Game Recommendation. Both motivation for future play and likelihood of game recommendation are measured using questions identical to a previous study . Motivation for future play showed good reliability, α=0.98.
To calculate a priori sample size, we perform two separate sample size determination calculations (both of these are specified in our preregistration at https://osf.io/dbvkp/). The first calculation is based on a 2 x 2 ANOVA for testing H1 and H2. G*Power 3.1 was used to perform this calculation using an effect size of small (0.1), α=0.05, and 95% power. G*Power 3.1 found that a sample size of N=1302 would be required .
For H3, H4, H5, H6, and H7, our sample size calculation is based on moderated mediation analyses. We performed Monte Carlo simulations in R. We first specify the complete model (i.e., containing X, M1, M2, M3, M4, W, and Y with the appropriate relationships) using the lavaan package, with parameter estimates of 0.1 (e.g., correlations between variables). We then use the simsem package to create Monte Carlo simulations using 1000 bootstraps.10 These simulations provided estimations of statistical power for each path, from which we use the lowest power value from all paths as the cutoff. We modified sample size iteratively (± 10) until the necessary power was reached. We performed 10 simulations to confirm that a specified sample size would reach the desired minimum power. The random number generator's seed was re-randomized for every simulation. These Monte Carlo simulations determined that, for a power of 95% and a confidence level of 95%, a sample size of 1500 would be necessary.11 Therefore, to ensure the necessary power across both sample size determinations (N=1302 and N=1500), we use N=1500. 
Two screenshots from the experiment. One image shows a robotic agent introducing the game to the player, along with controls for playing the game. The other image shows the in-game survey that players complete after 10 minutes of gameplay.
We recruited 1527 (47.6% female, 1.2% gender variant, 0.4% transgender) participants with an average age of M=37.26 (SD=11.14) from MTurk.12 Workers on MTurk complete Human Intelligence Tasks (HITs), including research experiments. Studies show that MTurk provides data of similar quality  as typical samples (e.g., college students). Participants were each paid $5.00 (USD). The HIT was available to workers in the U.S. over the age of 18 who had a computer with working audio. For quality control, workers were required to have a HIT approval rate >95%. The Purdue University Institutional Review Board (IRB) approved the study. All participants were asked to provide informed consent.
4.9.1 Data Screening. We screened all participants’ responses. Specifically, we carefully screened participants’ who had at least three survey measures with zero variance (excluding likelihood of game recommendation, since this was only a single question) or with ± 3SD. A fairly large number of respondents met the criteria of at least three survey measures with zero variance (~40%),13 and these responses were scrutinized further (e.g., reverse-coded items and open-ended questions). All responses were deemed legitimate, except for one respondent who responded to all questions (including reverse-coded items) with the same answer. This respondent was removed from further analysis (N=1526 remaining participants).
4.9.2 Experience With Video Games and Programming. Participants reported playing an average of M=8.5 (SD=10.5) hours of video games per week, approximately matching the global average of M=8.45 =0.263, p=0.852, η2pηp2=0.001).
A between-subjects factorial design was used. Each participant was randomly assigned to one of four possible conditions. Participant counts in each condition were approximately equal (M=381.5, SD=5.8).
Participants first filled out an IRB-approved consent form. Participants were informed that they could exit the game at any time after playing 10 minutes. Participants then began playing CodeBreakers. At the beginning of the game, participants underwent an audio check during which they were required to type a spoken English word. Participants then used the avatar customization interface corresponding to their condition. A robotic agent then engaged in a short conversation with the player. The robot was animated with audio dialogue generated through an automatic voice generator . After a brief introduction, the participant was provided instructions on how to play the game. See Figure 6 a. Participants were told they could exit the game at any time after playing 10 minutes by pressing ESC on their keyboard, then clicking quit game. The participant then began playing the game. During gameplay, the text “Time Remaining for Survey” appeared at the top of the screen, with a countdown timer starting from 10 minutes. Once the 10 minutes had elapsed, participants were automatically presented an in-game survey which contained the PIS, PENS autonomy, IMI interest/enjoyment, PXI immersion, motivation for future play, and likelihood of game recommendation questions. See Figure 6 b. All participant game data was automatically logged (e.g., time played, avatar customization choices). After the survey was completed, a message box appeared, reminding participants that they could now quit at any time, and that they could continue playing for as long as they liked. The message at the top of the game screen which had shown the time remaining was replaced by the message “You may play for as long as you like and quit at any time by pressing ESC and clicking Quit Game.” Once participants quit the game (or completed all 6 levels), participants were then asked to describe in their own words any problems encountered. Participants then filled out a set of questions about prior video game experience, programming experience, and demographics.
Data was analyzed using SPSS 23 and the PROCESS macro for SPSS . The parallel mediation was repeated using the different outcomes of interest (Y): PXI immersion (H4); time spent playing (H5); motivation for future play (H6); and likelihood of game recommendation (H7). In order to perform exploratory analyses on whether audial choice (W) moderates paths (direct and indirect) between X and Y, we used PROCESS model 59. We used an α of 0.05. These analyses were all preregistered at https://osf.io/dbvkp/. 
To ensure that there were no effects of a specific model, or a specific voice, on collected measures (see Section 4.7 for our measures), we used one-way MANOVA. First, we grouped all participants who were assigned a model randomly—i.e., participants in the Choice-None and Choice-Audio conditions. Second, we created another group of participants who were assigned a voice randomly—i.e., participants in the Choice-None and Choice-Visual conditions. We only chose participants who were assigned an avatar or voice randomly (and not through choice), since this gives the best approximation of how an avatar or voice may influence a player while avoiding the confound of a self-selection effect. Using the two groups, we then ran two MANOVAs with the IVs of either avatar (group 1) or voice (group 2) and the DVs of our collected measures. Prior to running our MANOVAs, we checked both assumption of homogeneity of variance and homogeneity of covariance by the test of Levene's Test of Equality of Error Variances and Box's Test of Equality of Covariance Matrices; and both assumptions were met by the data (p>0.05 for Levene's, and p>0.001 for Box's). However, Levene's test was violated for the measure of time played in the MANOVA for group 2 (p<0.05). To deal with this violation, we used the more conservative Pillai's Trace =1.241, p=0.183, Pillai's Trace=0.044, η2pηp2=0.015. Therefore, when assigned randomly, neither a specific model nor a specific voice had a significant effect on our measures.
From Table 2, factorial 2 x 2 ANOVAs (choice visual x choice audial) found main effects of choice visual on similarity identification, embodied identification, and wishful identification (H1.1 supported). In contrast, there were no main effects of choice audial (H1.2 not supported). However, a significant interaction effect was found between choice visual and choice audial on similarity identification, embodied identification, and wishful identification (H1.3 not supported). Significant interaction effects were further probed through a simple effects analysis. As this involved two additional tests, the significance threshold was Bonferroni-adjusted to p=0.025. Simple effects analysis found that in all cases, the effect of choice audial when there was no visual choice was not significant. However, in all cases, the effect of choice audial when there was visual choice was significant and positive. Therefore, in the absence of a visual avatar choice, choice of avatar voice has no effect, but in the presence of a visual avatar choice, choice of avatar voice has a significantly positive effect on similarity identification, embodied identification, and wishful identification. Effect sizes (η2pηp2) are in the small-to-medium (0.01 to 0.09) range for main effects of choice visual, and small (0.01) for interaction effects.15 
From Table 2, a factorial 2 x 2 ANOVA (choice visual x choice audial) found a main effect of choice visual on autonomy (H2.1 supported). In contrast, there was no main effect of choice audial (H2.2 not supported). However, a significant interaction effect was found between choice visual and choice audial on autonomy (H1.3 not supported). The significant interaction effect was further probed through a simple effect analysis. As this involved two additional tests, the significance threshold was Bonferroni-adjusted to p=0.025. The simple effect analysis found that the effect of choice audial when there was no visual choice was not significant. However, the effect of choice audial when there was visual choice was significant and positive. Therefore, in the absence of a visual avatar choice, choice of avatar voice has no effect, but in the presence of a visual avatar choice, choice of avatar voice has a significantly positive effect on autonomy. The effect size (η2pηp2) is considered small. 
5.4.1 Assumption Checks. Mediation analyses require several important assumptions to be met , and we found no outliers. Therefore, our mediation analysis assumptions are met.
5.4.2 Hypothesis Tests.

Moderated mediation model being tested in our exploratory analyses. Audial Avatar Choice (W) as potentially moderating the paths in Figure 7.
The mediation model being tested can be seen in Figure 7. From Table 3, we can see that visual choice has a direct effect (c′) on time spent playing only (H5.1 supported; H3.1, H4.1, H6.1, and H7.1 not supported). A 95% bias-corrected confidence interval based on 10,000 bootstrap samples indicates several significant indirect effects on intrinsic motivation (a2b2 and a3b318 supporting H3.2, a4b4 supporting H3.3), immersion (a2b2 supporting H4.2, a4b4 supporting H4.3), time spent playing (a1b1 and a3b3 supporting H5.2, H5.3 not supported), motivation for future play (a2b2 and a3b3 supporting H6.2, a4b4 supporting H6.3), and likelihood of game recommendation (a2b2 and a3b3 supporting H7.2, a4b4 supporting H7.3). Therefore, we conclude that visual choice directly affects time spent playing, and indirectly affects intrinsic motivation (via embodied identification, wishful identification, and autonomy), immersion (via embodied identification and autonomy), time spent playing (via similarity identification and wishful identification), motivation for future play (via embodied identification, wishful identification, and autonomy), and likelihood of game recommendation (via embodied identification, wishful identification, and autonomy). Descriptives for each variable can be seen in Table 4.
5.4.3 Exploratory Analyses.

For the exploratory analyses with no a priori hypotheses, we test the model seen in Figure 8. Results of the moderated mediation are found in Table 5. We find evidence of significant moderated mediation through the moderator of audial choice for intrinsic motivation (moderating X → M1 → Y and X → M4 → Y), immersion (moderating X → M2 → Y and X → M4 → Y), time spent playing (moderating X → Y), motivation for future play (moderating X → M3 → Y and X → M4 → Y), and likelihood of game recommendation (moderating X → M4 → Y). These effects were then probed while fixing the value of audial choice to 0 or 1 (see Table 5). When these effects were probed while fixing audial choice to 0, the mediations in all cases were non-significant. On the other hand, when fixing audial choice to 1, the mediations in all cases were positive and significant. Therefore, audial choice positively moderates different paths across all outcome variables.19,20 
Existing work on avatar customization has focused almost exclusively on visual aspects of customization. While there are many benefits to avatar customization, it is unknown whether audial avatar customization confers similar benefits.
We conducted a 2 x 2 (visual choice x audial choice) experiment. Visual customization directly increases avatar identification and autonomy. Visual customization directly increases time spent playing and indirectly (through avatar identification and autonomy as mediators) increases intrinsic motivation,21 immersion, time spent playing, motivation for future play, and likelihood of game recommendation. Audial customization did not lead to a direct increase in avatar identification and autonomy. A significant interaction effect showed that audial customization directly increases avatar identification and autonomy, but only when visual customization was also available. Audial customization significantly moderated eight paths between visual customization and the outcome variables intrinsic motivation, immersion, time spent playing, motivation for future play, and likelihood of game recommendation. The moderation was such that when audial customization was unavailable, the path had a non-significant effect on the outcome, but when audial customization was available, the path had a significant effect on the outcome. Based on these results, we conclude that audial customization plays an important role in affecting outcomes.
However, we make the argument that although audial customization is important, it appears to have a weaker effect in comparison to visual customization. This argument is based on two facets of the results: (1) visual customization alone has a significant effect on avatar identification and autonomy, whereas audial customization has a significant effect only within the group of participants who also have visual customization available;22 and (2) audial customization's effects on avatar identification and autonomy have lower effect sizes (small) when compared to visual customization (small-to-medium) . The first point suggests that audial customization plays an enhancing role for visual customization (i.e., when visual customization was present, audial customization further increased avatar identification and autonomy compared to no audial customization). Both points together suggest that audial customization, although important, is somewhat weaker than visual customization.
Many possibilities exist for why visual customization had a stronger effect than audial customization. One possibility is that players are simply more familiar with visual customization. People are known to prefer things due to familiarity alone. The familiarity principle (also called the mere-exposure effect) describes the phenomenon of preference for things merely due to familiarity . Therefore, the effects of visual customization could have been enhanced through familiarity.
Additionally, the total exposure time to the audial customization aspects of the avatar (i.e., voice) was only a fraction of the exposure time to the visual aspects of the avatar (i.e., model). While the audial aspect of the avatar is infrequent and typically only occurs before and after each puzzle, the visual aspect of the avatar is always present on screen. Moreover, the audial aspect of the avatar was interleaved with other sounds (game audio and background noise). Such factors could have all served to reduce the impactfulness of audial customization. Studying games with frequent voice lines (e.g., a narrative adventure such as The Walking Dead ) would help to balance the exposure between visual and audial aspects of the avatar. Such studies would help to understand if the reason for the discrepancy between visual and audial customization effects stems from exposure.
Visual aspects of an avatar might also inherently (at a fundamental level) be more important than audial aspects. Humans have been shown to have better visual memory than auditory memory and that there appear to be fundamental differences between visual and auditory processing . Reasons for why the picture superiority effect happens are still being debated. However, this fundamental asymmetry between visual and auditory stimuli would give credibility to the argument that visual aspects of an avatar are inherently more important than audial aspects of the avatar.
It may also be possible to explain the audial-visual discrepancy through investment of effort. If participants view the visual aspect of their avatar as more important, then they may invest more effort into visual customization than audial customization. According to Cialdini's commitment and consistency principle, people tend to behave in ways consistent with how they have acted in the past  (i.e., future behavior often resembles past behavior). To maintain consistency with the effort in customizing the avatar visually, players would also invest more effort into the game. This would increase outcomes (e.g., avatar identification). Future studies could study the customization process itself more closely—e.g., time spent on customizing visual vs. audial aspects, measuring cognitive load in customizing visual vs. audial aspects.
Interestingly, audial customization was only effective at increasing avatar identification and autonomy when visual customization was also present. This was true even when we re-performed all analyses with only participants with a matching avatar gender (see footnote 20). The reason for this is not immediately apparent. Although the character customization conditions were designed carefully and validated with expert UI designers, it is possible that the ability to customize voice (and especially in the absence of model selection) did not match players’ expectations. A more in-depth investigation into the avatar customization process itself may help shed light on this phenomenon. Based on our results, we recommend pairing audial customization options with visual customization options to enhance outcomes.
This research has examined both the effects of avatar customization (e.g., ).
The finding that audial choice significantly moderates the effect of visual choice on game outcomes (as mediated by identification and autonomy) provides further evidence for the importance of audial avatar customization. For example, visual customization was associated with greater intrinsic motivation (finding the game satisfying), sense of immersion, motivation for future play, and game recommendation, but only when there was audial customization, and all of these associations were fully mediated by embodied identification. In other words, visual customization alone did not sufficiently induce an association between embodied identification and these game outcomes, but visual together with audial customization did. Similarly, visual and audial customization together induced greater time spent playing the game and this effect was partially mediated by similarity identification. Together, these findings suggest that audial customization is a notable contributor not only to the subjective experience of identification with the avatar, but also to the outcomes of identification with the avatar within the game.
This work is also relevant to the Proteus Effect, a phenomenon whereby users tend to conform to the expected behaviors of their avatars . However, allowing users to create audial avatar identities could also be a powerful avenue for inducing the Proteus Effect. In the present context of learning games, this research suggests that using an avatar with a voice that sounds more capable of success in a computer-science context (e.g., intelligent, persistent) might empower players to perform better in the game and thus learn the educational content more effectively. Further research could be designed to confirm this expectation first by pretesting the perceived intelligence/persistence of different voices and then assigning exemplary voices as customization options within a similar game.
The amount of dialogue in CodeBreakers can be considered minimal compared to most games that contain voiced dialogue—e.g., Mass Effect .
Audial customization could enhance video instruction . Further research is needed on audial customization to understand more generally the potential use cases.
Despite the robust design of this controlled experiment, there are some limitations to the study's external and internal validity that should be considered in future research. First, participants were given only two visual and audial customization choices for each gender. Many games provide a greater number of choices during avatar customization, suggesting that avatar identification in such games is generally higher than it was in our study. Further, participants were likely more familiar with visual avatar customization than audial customization given that the former is more prevalent in current games and social media. Hence, the choice of avatar appearance—even based on just two options—was more likely to remind participants of previous avatar customization experiences that involved choices over many visual aspects of an avatar. In contrast, audial customization could potentially include a wide range of avatar characteristics that were not included in the present study (e.g., footsteps, whistling, grunting noises, pitch modification), but the participants’ choice of avatar voice was less likely to remind them of these possibilities. Moreover, avatar identification may have been limited for players who do not conform to stereotypical representations of “male” and “female” voices. For these reasons, future research on this topic should include a larger set of customization options, especially for audial avatar characteristics.
The study also included a potential confound relating to the attention paid to audial and visual cues. Namely, in order to proceed in the game, participants were required to solve visual puzzles that did not include audial elements. This prioritization of visual stimuli may have led to a greater focus on the avatar's appearance compared to avatar's speech, partially explaining why visual avatar customization was more consequential in the study outcomes. Another related but minor issue is that the quality of the sound hardware may have varied between players’ computers causing noise in the data (i.e., less attention to audial cues), but this was likely not confounded with experiment condition given random assignment. Further, all participants performed an audio check, so a threshold of audial attention can be inferred.
The study relied on participants being paid to play the game, like most research in this field, which potentially limits ecological validity. Further, generalizability was not established beyond the single, education-oriented game designed for this research. Relatedly, the study cannot determine how specific facets of this particular game design (e.g., pacing) influenced the study outcomes. For one, the game was designed to highlight the avatar's voice for a single user, so the study findings do not directly speak to multi-user games which offer voice-based communication . The present findings indirectly suggest that customizing such voice modification might also be beneficial to the user's experience in other ways.
The study required participants to play the game for a minimum of 10 minutes, which is significantly less time than many people tend to play video games , and the present study was not intended to examine changes in identification over time. We should also note that 10-minute exposures are common in video-game experiments, perhaps due to operational constraints, but these studies tend to find sufficient effects on their outcomes of interest with such durations.
Avatar customization is known to positively affect crucial outcomes in numerous domains, including health, entertainment, and education. However, studies on avatar customization have focused almost exclusively on visual aspects of customization. It is unknown whether audial customization can confer the same benefits as visual customization. We presented one of the first studies to date on audial avatar customization. Participants with visual choice experienced higher avatar identification and autonomy. Participants with audial choice experienced higher avatar identification and autonomy, but only within the group of participants who had visual choice available. Visual choice led to an increase in time spent and indirectly led to increases in intrinsic motivation, immersion, time spent, future play motivation, and likelihood of game recommendation. Audial choice moderated the majority of these effects. Our results suggest that audial customization, although having a moderately weaker effect compared to visual customization, plays an important role in enhancing all outcomes compared to visual customization alone. We discussed the implications for research and potential applications of audial avatar customization. This work takes an important first step in developing a baseline understanding of audial avatar customization.
